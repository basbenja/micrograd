{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05823364",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1289e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from Value import Value\n",
    "from utils import draw_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044c44c",
   "metadata": {},
   "source": [
    "Veamos el siguiente bug que ocurre en la implementación actual de nuestro backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a; b.label = 'b'\n",
    "b.backward()\n",
    "draw_trace(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7c6a4",
   "metadata": {},
   "source": [
    "El grafo no muestra todo, en realidad, hay dos nodos con el label 'a' solapados. Lo que sí está mal\n",
    "es que el gradiente de `b` con respecto a `a` sea 1. Si lo calculamos analíticamente, vemos que\n",
    "debería ser 2:\n",
    "\n",
    "Si $b = f(a) = a + a$, entonces $\\partial{b}/\\partial{a} = f'(a) = 1 + 1 = 2$\n",
    "\n",
    "Veamos por qué ocurre esto:\n",
    "`b` es el resultado de una suma, entonces al llamar a `b._backward()` (esto ocurre al llamar a `b.backward()`), se está llamando a la siguiente función:\n",
    "```python\n",
    "def _backward():\n",
    "    self.grad = 1.0 * out.grad\n",
    "    other.grad = 1.0 * out.grad\n",
    "```\n",
    "\n",
    "En este caso, `out.grad` es 1 (esto se sete al llamar `b.backward()`). Y lo que pasa es que `self` y `other` son ambos el mismo `Value` (que es `a`), entonces\n",
    "estamos sobreescribiendo el gradiente.\n",
    "\n",
    "Veámoslo con una expresión más complicada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e419f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b; d.label = 'd'\n",
    "e = a + b; e.label = 'e'\n",
    "f = d * e; f.label = 'f'\n",
    "\n",
    "f.backward()\n",
    "\n",
    "draw_trace(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210613d2",
   "metadata": {},
   "source": [
    "Calculemos los gradientes de `a` y `b` analíticamente:\n",
    "\n",
    "Tenemos que:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f &= d * e \\\\\n",
    "      &= (a * b) * (a + b)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Entonces, tenemos que aplicar la regla de la cadena viendo a f como $ f(d(a,b), (e(a,b)))$. Estamos en este caso: [Chain rule - Case of scalar-valued functions with multiple inputs](https://en.wikipedia.org/wiki/Chain_rule#Case_of_scalar-valued_functions_with_multiple_inputs).\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial{f}}{\\partial{a}} &= \\frac{\\partial{f}}{\\partial{d}}\\frac{\\partial{d}}{\\partial{a}} + \\frac{\\partial{f}}{\\partial{e}}\\frac{\\partial{e}}{\\partial{a}} \\\\\n",
    "    &= e * b + d * 1 \\\\\n",
    "    &= e * b + d \\\\\n",
    "    &= 1 * 3 + (-6) \\\\\n",
    "    &= -3\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e6e68",
   "metadata": {},
   "source": [
    "Los gradientes de `e` y `d` están bien. Los que están mal son los de `a` y `b`, esta vez porque se está usando la misma referencia en dos operaciones distintas. Entonces, supongamos que primero se calcula el gradiente de `a` y `b` pasando por `e`, pero después al hacer lo mismo pasando por `d`, los valores anteriores se sobreescriben.\n",
    "\n",
    "Entonces, básicamente lo que va a pasar es que siempre que usemos un mismo `Value` más de una vez, vamos a tener un error.\n",
    "\n",
    "La solución a esto es ir acumulando el gradiente, como explica en [Chain rule - Multivariable case](https://en.wikipedia.org/wiki/Chain_rule#Multivariable_case).\n",
    "\n",
    "Lo aplicamos en la clase `Value` y si corremos este código de nuevo, vamos a ver que ahora los gradientes dan bien."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
